document = doc_id,
term = lemma,
value = n
)
dtm <- removeSparseTerms(dtm, .9)
rownames(dtm) <- blogs$doc_id
dtm.1 <- dtm[rownames(dtm) %in% blogs[blogs$day <= 180, "doc_id"],]
dtm.2 <- dtm[rownames(dtm) %in% blogs[blogs$day > 180, "doc_id"],]
terms.1 <- t(as.matrix(dtm.1)) %*% as.matrix(dtm.1)
terms.2 <- t(as.matrix(dtm.2)) %*% as.matrix(dtm.2)
diag(terms.1) <- 0
diag(terms.2) <- 0
terms.1 <- terms.1/rowSums(terms.1)
terms.2 <- terms.2/rowSums(terms.2)
causative <- solve(terms.1) %*% terms.2
dtm
dtm <- df %>%
cast_dtm(
document = doc_id,
term = lemma,
value = n
)
dtm <- removeSparseTerms(dtm, .85)
rownames(dtm) <- blogs$doc_id
dtm.1 <- dtm[rownames(dtm) %in% blogs[blogs$day <= 180, "doc_id"],]
dtm.2 <- dtm[rownames(dtm) %in% blogs[blogs$day > 180, "doc_id"],]
terms.1 <- t(as.matrix(dtm.1)) %*% as.matrix(dtm.1)
terms.2 <- t(as.matrix(dtm.2)) %*% as.matrix(dtm.2)
diag(terms.1) <- 0
diag(terms.2) <- 0
terms.1 <- terms.1/rowSums(terms.1)
terms.2 <- terms.2/rowSums(terms.2)
causative <- solve(terms.1) %*% terms.2
temp <- causative[order(causative$clinton),]
cause.temp <- as.data.frame(causative)
temp <- causative[order(cause.temp$clinton),]
temp <- cause.temp[order(cause.temp$clinton),]
cause.temp$clinton)
cause.temp$clinton
temp <- cause.temp[order(cause.temp$obama),]
cause.temp[order(cause.temp$obama),] %>%
rownames()
cause.temp[order(cause.temp$bush),] %>%
rownames()
data("corpus_taylor_swift", package = "text2map.corpora")
corpus_taylor_swift$song_text
?perm_tester
library(text2map)
?text2map::perm_tester()
View(corpus_taylor_swift)
lm(song_annotation_count ~ song_pageviews, data = corpus_taylor_swift) |> summary()
lm(log(song_annotation_count+1) ~ log(song_pageviews+1), data = corpus_taylor_swift) |> summary()
untar("Downloads/twitter.tar.gz", exdir = "Downloads/")
library(igraph)
alters <- read.table("Downloads/twitter/12831.edges")
alter.features <- read.table("Downloads/twitter/12831.feat", row.names = 1)
features.names <- read.table("Downloads/twitter/12831.featnames",
sep = " ",
comment.char = "*",
quote = "",
stringsAsFactors = F,
row.names = 1)
ego.features <- read.table("Downloads/twitter/12831.egofeat")
colnames(alter.features) <- t(features.names)
colnames(ego.features) <- t(features.names)
ego.net <- graph_from_edgelist(as.matrix(alters))
ego.net
vcount(ego.net)
ecount(ego.net)
el <- matrix( c("foo", "bar", "bar", "foobar"), nc = 2, byrow = TRUE)
el
View(alters)
ego.net <- from_edgelist(as.matrix(alters))
ego.net
graph_
ego.net <- graph_from_edgelist(as.matrix(alters))
temp <- as.matrix(alters)
View(temp)
dim(as.matrix(alters))
View(alters)
View(alter.features)
class(temp[,1])
class(temp[,2])
temp <- as.matrix(alters)
temp[,1] <- as.character(temp[,1])
temp[,2] <- as.character(temp[,2])
View(temp)
ego.net <- graph_from_edgelist(temp)
ego.net
plot(ego.net)
alters <- as.matrix(alters)
alters[,1] <- as.character(alters[,1])
alters[,2] <- as.character(alters[,2])
ego.net <- graph_from_edgelist(alters)
ego.net
table(unique(alters))
sum(unique(alters))
count(unique(alters))
unique(alters)
table(unique(alters))
table(unique(alters)) |> sum()
table(unique(alters)) |> dim()
alter.attributes$name <- rownames(alter.attributes)
for (i in 2:length(alter.attributes)){
# Select attributes
temp_attribute <- alter.attributes[,c(1,i)]
# Match the attribute id with the id of the graph vertexes
sorted_attr <- temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 2]
# Set attributes
ego.net <- set_vertex_attr(ego.net, name = (names(alter.attributes)[i]),
index = V(ego.net), value = sorted_attr)
}
# Read in the vertex attributes for the alters. Note that all of them are
# binary and simply indicate whether that word, hashtag, etc. is used
# on alter's profile or not
alter.attributes <- read.table("Downloads/twitter/12831.feat", row.names = 1)
# The names of the attributes
attribute.names <- read.table("Downloads/twitter/12831.featnames",
sep = " ",
comment.char = "*",
quote = "",
stringsAsFactors = F,
row.names = 1)
# Ego's attributes, though you probably don't need these
ego.attributes <- read.table("Downloads/twitter/12831.egofeat")
colnames(alter.attributes) <- t(attribute.names)
colnames(ego.attributes) <- t(attribute.names)
alter.attributes$name <- rownames(alter.attributes)
for (i in 2:length(alter.attributes)){
# Select attributes
temp_attribute <- alter.attributes[,c(1,i)]
# Match the attribute id with the id of the graph vertexes
sorted_attr <- temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 2]
# Set attributes
ego.net <- set_vertex_attr(ego.net, name = (names(alter.attributes)[i]),
index = V(ego.net), value = sorted_attr)
}
i
alter.attributes[,c(1,i)]
# load in your package
pacman::p_load(igraph,
tidyverse,
install = T)
alter.attributes <- relocate(alter.attributes$name)
alter.attributes <- alter.attributes %>%
relocate(name)
alter.attributes
View(alter.attributes)
for (i in 2:length(alter.attributes)){
# Select attributes
temp_attribute <- alter.attributes[,c(1,i)]
# Match the attribute id with the id of the graph vertexes
sorted_attr <- temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 2]
# Set attributes
ego.net <- set_vertex_attr(ego.net, name = (names(alter.attributes)[i]),
index = V(ego.net), value = sorted_attr)
}
temp_attribute[,1]
V(ego.net)$name
temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 2]
sorted_attr
length(sorted_attr)
table(duplicate(alter.attributes))
table(duplicated(alter.attributes))
sorted_attr
244-8
table(is.na(alters))
temp <- alter.attributes[alter.attributes$name == "8479062",]
View(temp)
temp <- alter.attributes[alter.attributes$name == "369246180",]
View(temp)
i
alter.attributes[,c(1,i)]
View(attribute.names)
temp_attribute
temp_attribute[,1]
V(ego.net)$name)
V(ego.net)$name
# Match the attribute id with the id of the graph vertexes
sorted_attr <- temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 2]
sorted_attr
V(ego.net)$name
alter.attributes
alter.attributes[alter.attributes$name == "8479062", "#,"]
alter.attributes[alter.attributes$name == "8479062", "#,,"]
alter.attributes[alter.attributes$name == "8479062", "#,"]
View(alter.attributes)
sorted_attr
V(ego.net)$name
length(sorted_attr)
temp_attribute
temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 2]
temp_attribute[match(temp_attribute[,1], V(ego.net)$name)]
temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 2]
table(is.na(sorted_attr))
temp_attribute
table(is.na(temp_attribute$`#,`))
V(ego.net)$name
table(is.na(alter.attributes$`#,`))
table(is.na(alter.attributes$name == "8479062"))
table(is.na(alter.attributes$name == "14305022"))
sorted_attr
table(is.na(alter.attributes$name == "398874773"))
any(alter.attributes$name == "398874773")
any(alter.attributes$name == "8479062")
temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 1]
table(is.na(alter.attributes$name))
names(alter.attributes)[i]
V(ego.net)
temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 1:2]
alter.attributes[,c(1,i)]
match(temp_attribute[,1], V(ego.net)$name)
View(temp_attribute)
2:length(alter.attributes)
alter.attributes[,c(1,i)]
sorted_attr <- temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 2]
temp_attribute[match(temp_attribute[,1], V(ego.net)$name), 1:2]
temp_attribute[,1]
V(ego.net)$name
# Match the attribute id with the id of the graph vertexes
sorted_attr <- temp_attribute[V(ego.net)$name, match(temp_attribute[,1]), 2]
# Match the attribute id with the id of the graph vertexes
sorted_attr <- temp_attribute[match(V(ego.net)$name, temp_attribute[,1]), 2]
temp_attribute[match(V(ego.net)$name, temp_attribute[,1]), 2]
temp_attribute[match(V(ego.net)$name, temp_attribute[,1]), 1:2]
# Select attributes
temp_attribute <- alter.attributes[,c(1,3)]
temp_attribute[match(V(ego.net)$name, temp_attribute[,1]), 2]
temp_attribute[match(V(ego.net)$name, temp_attribute[,1]), 1:2]
alter.attributes[alter.attributes$name == "1656891", 3]
alter.attributes[alter.attributes$name == "5827292", 3]
alter.attributes[alter.attributes$name == "13538092", 3]
# Unzip file
untar("Downloads/twitter.tar.gz", exdir = "Downloads/")
# Read in an edgelist containing the alters (vertices) that ego follows,
# and the connections between those alters
# Note that I chose this ego at random
alters <- read.table("Downloads/twitter/12831.edges")
# Read in the vertex attributes for the alters. Note that all of them are
# binary and simply indicate whether that word, hashtag, etc. is used
# on alter's profile or not
alter.attributes <- read.table("Downloads/twitter/12831.feat", row.names = 1)
# The names of the attributes
attribute.names <- read.table("Downloads/twitter/12831.featnames",
sep = " ",
comment.char = "*",
quote = "",
stringsAsFactors = F,
row.names = 1)
# Ego's attributes, though you probably don't need these
ego.attributes <- read.table("Downloads/twitter/12831.egofeat")
# Attach the atrribute names to alters' attributes themselves (so you know
# what they are)
colnames(alter.attributes) <- t(attribute.names)
colnames(ego.attributes) <- t(attribute.names)
# Create follow network among ego's alters
alters <- as.matrix(alters)
alters[,1] <- as.character(alters[,1])
alters[,2] <- as.character(alters[,2])
ego.net <- graph_from_edgelist(alters)
# Attach the attributes to the network
alter.attributes$name <- rownames(alter.attributes)
alter.attributes <- alter.attributes %>%
relocate(name)
for (i in 2:length(alter.attributes)){
# Select attributes
temp_attribute <- alter.attributes[,c(1,)]
# Match the attribute id with the id of the graph vertexes
sorted_attr <- temp_attribute[match(V(ego.net)$name, temp_attribute[,1]), 2]
# Set attributes
ego.net <- set_vertex_attr(ego.net, name = (names(alter.attributes)[i]),
index = V(ego.net), value = sorted_attr)
}
ego.net
i
sorted_attr
for (i in 2:length(alter.attributes)){
# Select attributes
temp_attribute <- alter.attributes[,c(1,i)]
# Match the attribute id with the id of the graph vertexes
sorted_attr <- temp_attribute[match(V(ego.net)$name, temp_attribute[,1]), 2]
# Set attributes
ego.net <- set_vertex_attr(ego.net, name = (names(alter.attributes)[i]),
index = V(ego.net), value = sorted_attr)
}
ego.net
set.seed(573)
plot(ego.net, vertex.color = "#ec008b", vertex.label.color = "black",
edge.color = adjustcolor("gray25", alpha.f = .8),
layout = layout.fruchterman.reingold,
edge.arrow.size = 0.7,
vertex.size = 6, vertex.label = NA)
set.seed(573)
plot(ego.net, vertex.color = "#ec008b", vertex.label.color = "black",
edge.color = adjustcolor("gray25", alpha.f = .8),
layout = layout.fruchterman.reingold,
edge.arrow.size = 0.2,
vertex.size = 6, vertex.label = NA)
set.seed(573)
plot(ego.net, vertex.color = "#ec008b", vertex.label.color = "black",
edge.color = adjustcolor("gray25", alpha.f = .8),
layout = layout.fruchterman.reingold,
edge.arrow.size = 0.2,
vertex.size = 2, vertex.label = NA)
set.seed(573)
plot(ego.net, vertex.color = "#ec008b", vertex.label.color = "black",
edge.color = adjustcolor("gray25", alpha.f = .1),
layout = layout.fruchterman.reingold,
edge.arrow.size = 0.2,
vertex.size = 2, vertex.label = NA)
set.seed(573)
plot(ego.net, vertex.color = "#ec008b", vertex.label.color = "black",
edge.color = adjustcolor("gray25", alpha.f = .2),
layout = layout.fruchterman.reingold,
edge.arrow.size = 0.2,
vertex.size = 2, vertex.label = NA)
gender <- matrix(c(0,0,1), nrow(3), ncol(3))
gender <- matrix(c(0,0,1), 3, 3)
View(gender)
library(sna)
?netlm
pacman::p_load(tidyverse, plm, permute, data.table,
marginaleffects, ggpubr, forcats,
freqtables, lme4, broom.mixed, psych,
fixest, modelsummary,
install = T)
#smart quotes/dashes when using newer
#versions. But need 7-3 for other features.
library(tm)
pacman::p_load(tidyverse, stm, here,
install = T)
data <- Corpus(DirSource(here("TXTs")))
data <- Corpus(DirSource(here("TXTs")))
here("TXTs")
setwd("/Users/mtaylor2/Library/CloudStorage/GoogleDrive-marshall.taylor89@gmail.com/My Drive/wno_attention")
data <- Corpus(DirSource(here("TXTs")))
here("TXTs")
pacman::p_load(tidyverse, plm, permute, data.table,
marginaleffects, ggpubr, forcats,
freqtables, lme4, broom.mixed, psych,
fixest, modelsummary,
install = T)
#smart quotes/dashes when using newer
#versions. But need 7-3 for other features.
library(tm)
pacman::p_load(tidyverse, stm, here,
install = T)
data <- Corpus(DirSource(here("TXTs")))
here("TXTs")
data <- file.path("/Users/mtaylor2/Library/CloudStorage/GoogleDrive-marshall.taylor89@gmail.com/My Drive/wno_attention/TXTs")
data <- Corpus(DirSource(data))
remove.packages(tm)
remove.packages("tm")
pacman::p_load(tidyverse, plm, permute, data.table,
marginaleffects, ggpubr, forcats,
freqtables, lme4, broom.mixed, psych,
fixest, modelsummary,
install = T)
packageurl <- "http://cran.r-project.org/src/contrib/Archive/tm/tm_0.6-2.tar.gz"
install.packages(packageurl, repos = NULL, type = "source") #Must use this version. Weird issue with
#smart quotes/dashes when using newer
#versions. But need 7-3 for other features.
library(tm)
pacman::p_load(tidyverse, stm, here,
install = T)
data <- Corpus(DirSource(here("TXTs")))
temp <- Corpus(DirSource("~/Downloads/TXTs"))
temp <- VCorpus(DirSource("~/Downloads/TXTs"))
?DirSource()
pacman::p_load(tidyverse,
tidytext,
quanteda,
tm,
textclean,
textstem,
textreg,
stringi,
udpipe,
text2map,
install = T)
install.packages("textreg")
library(textreg)
remove.packages("tm")
pacman::p_load(tidyverse,
tidytext,
quanteda,
tm,
textclean,
textstem,
stringi,
udpipe,
text2map,
install = T)
library(textreg)
pacman::p_load(tidyverse,
tidytext,
quanteda,
tm,
readtext,
textreg,
textclean,
textstem,
stringi,
udpipe,
text2map,
stm,
Hmisc,
install = T)
remotes::install_gitlab("culturalcartography/text2map.dictionaries")
library(text2map.dictionaries)
data <- readtext(paste0(getwd(), "/TXTs/*"))
pre.meta <- readRDS("pre_topic_meta.rds")
data("concreteness", package = "text2map.dictionaries")
#pre STM cleaning
data <- data %>%
#transliterate
mutate(text = stri_trans_general(text,
id = "Any-Latin;Latin-ASCII"),
#lowercase
text = tolower(text),
#replace curly quotes
text = replace_curly_quote(text),
#remove punctuation external to a word group
text = gsub("(\\w+[_'-]+\\w+)|[[:punct:]]+", "\\1", text),
#replace contractions
text = replace_contraction(text),
#remove numbers
text = gsub("[[:digit:]]+", " ", text),
#remove excess whitespace between words
text = gsub("[[:space:]]+", " ", text),
#remove excess whitespace at begining and end of strings
text = trimws(text))
View(data)
data <- readtext(paste0(getwd(), "/TXTs/*"))
#pre STM cleaning
data <- data %>%
#transliterate
mutate(text = stri_trans_general(text,
id = "Any-Latin;Latin-ASCII"),
#lowercase
text = tolower(text),
#replace curly quotes
text = replace_curly_quote(text),
#remove punctuation external to a word group
text = gsub("(\\w+[_'-]+\\w+)|[[:punct:]]+", "\\1", text),
#replace contractions
text = replace_contraction(text),
#remove numbers
text = gsub("[[:digit:]]+", " ", text),
#remove excess whitespace between words
text = gsub("[[:space:]]+", " ", text),
#remove excess whitespace at begining and end of strings
text = trimws(text))
pre.meta <- pre.meta[order(match(rownames(pre.meta), data$doc_id)),]
identical(rownames(pre.meta), data$doc_id) #should be true
table(duplicated(data$doc_id))
table(duplicated(data))
data <- unique(data)
pre.meta <- pre.meta[order(match(rownames(pre.meta), data$doc_id)),]
identical(rownames(pre.meta), data$doc_id) #should be true
pre.meta <- pre.meta[order(match(rownames(pre.meta), data$doc_id)),]
identical(rownames(pre.meta), data$doc_id) #should be true
#pre STM cleaning
data <- data %>%
#transliterate
mutate(text = stri_trans_general(text,
id = "Any-Latin;Latin-ASCII"),
#lowercase
text = tolower(text),
#replace curly quotes
text = replace_curly_quote(text),
#remove punctuation external to a word group
text = gsub("(\\w+[_'-]+\\w+)|[[:punct:]]+", "\\1", text),
#replace contractions
text = replace_contraction(text),
#remove numbers
text = gsub("[[:digit:]]+", " ", text),
#remove excess whitespace between words
text = gsub("[[:space:]]+", " ", text),
#remove excess whitespace at begining and end of strings
text = trimws(text))
pre.meta <- pre.meta[order(match(rownames(pre.meta), data$doc_id)),]
identical(rownames(pre.meta), data$doc_id) #should be true
rownames(pre.meta)
data <- readtext(paste0(getwd(), "/TXTs/*"))
pre.meta <- readRDS("pre_topic_meta.rds")
data <- unique(data)
colnames(data)
pre.meta <- pre.meta[order(match(rownames(pre.meta), data$doc_id)),]
identical(rownames(pre.meta), data$doc_id) #should be true
getwd()
data <- readtext(paste0(getwd(), "/TXTs/*"))
pre.meta <- readRDS("pre_topic_meta.rds")
View(pre.meta)
pre.meta <- pre.meta[order(match(rownames(pre.meta), data$doc_id)),]
rownames(pre.meta)
data$doc_id
order(match(rownames(pre.meta), data$doc_id))
pacman::p_load(tidyverse, plm, permute, data.table,
marginaleffects, ggpubr, forcats,
freqtables, lme4, broom.mixed, psych,
fixest, modelsummary,
install = T)
pre.meta <- pre.meta[order(match(rownames(pre.meta), data$doc_id)),]
rownames(pre.meta)
View(pre.meta)
data$doc_id
rm(data)
getwd()
data <- readtext(paste0(getwd(), "/TXTs/*"))
data$doc_id
remove.packages("readtext")
